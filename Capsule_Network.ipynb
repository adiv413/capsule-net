{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f32a76d1f0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import resources\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# random seed (for reproducibility)\n",
    "seed = 1\n",
    "# set random seed for numpy\n",
    "np.random.seed(seed)\n",
    "# set random seed for pytorch\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# convert data to Tensors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.CIFAR10(root='data', train=True,\n",
    "                            download=True, transform=transform)\n",
    "\n",
    "test_data = datasets.CIFAR10(root='data', train=False, \n",
    "                           download=True, transform=transform)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size=batch_size, \n",
    "                                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = torch.from_numpy(images.numpy())\n",
    "\n",
    "# # plot the images in the batch, along with the corresponding labels\n",
    "# fig = plt.figure(figsize=(25, 4))\n",
    "# for idx in np.arange(batch_size):\n",
    "#     ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
    "#     ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "#     # print out the correct label for each image\n",
    "#     # .item() gets the value contained in a Tensor\n",
    "#     ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initial convolutional layer: we're running 256 kernels of size 9x9 on the image\n",
    "# This generates one 24x24 (if using CIFAR) feature map with 256 channels \n",
    "# https://datascience.stackexchange.com/questions/64278/what-is-a-channel-in-a-cnn\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels=256, kernel_size=9, stride=1, padding=0):\n",
    "        '''Constructs the ConvLayer with a specified input and output size.\n",
    "            param in_channels: input depth of an image, default value = 1\n",
    "            param out_channels: output depth of the convolutional layer, default value = 256\n",
    "            param kernel_size: size of the convolutional kernel, default value = 9\n",
    "            param stride: stride of the convolutional kernel, default value = 1\n",
    "            param padding: zero padding added to the input, default value = 0\n",
    "           '''\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        # defining a convolutional layer of the specified size\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                              kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Defines the feedforward behavior.\n",
    "           param x: the input to the layer; an input image\n",
    "           return: a relu-activated, convolutional layer\n",
    "           '''\n",
    "        # applying a ReLu activation to the outputs of the conv layer\n",
    "        raw_output = self.conv(x)\n",
    "        features = F.relu(raw_output) # will have dimensions (batch_size, n - 8, n - 8, 256), where n is the size of the original image\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=9, stride=2, padding=0):\n",
    "        '''Constructs a list of convolutional layers to be used in \n",
    "           creating capsule output vectors.\n",
    "            param num_capsules: number of capsules to create\n",
    "            param in_channels: input depth of features, default value = 256\n",
    "            param out_channels: output depth of the convolutional layers, default value = 32\n",
    "            param kernel_size: size of the convolutional kernel, default value = 9\n",
    "            param stride: stride of the convolutional kernel, default value = 2\n",
    "            param padding: zero padding added to the input, default value = 0\n",
    "           '''\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "        # creating a list of convolutional layers for each capsule I want to create\n",
    "        # all capsules have a conv layer with the same parameters\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                      kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "            for _ in range(num_capsules)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''Defines the feedforward behavior.\n",
    "           param x: the input; features from a convolutional layer\n",
    "           return: a set of normalized, capsule output vectors\n",
    "           '''\n",
    "        # get batch size of inputs\n",
    "        batch_size = x.shape[0]\n",
    "  \n",
    "        # the capsule network array ouptuts batch_size number of feature maps that are\n",
    "        # (n - (kernel_size - 1)) / stride by (n - (kernel_size - 1)) / stride\n",
    "        # and have out_channels number of channels\n",
    "        \n",
    "        # reshape the the capsule net array outputs to be (batch_size, TOTAL_NUMBERS_IN_FEATURE_MAP, 1)\n",
    "        # this total number of numbers in feature map is (n - (kernel_size - 1)) / stride * (n - (kernel_size - 1)) / stride * out_channels\n",
    "        # THE ABOVE NUMBER MUST BE EDITED IF PADDING IS CHANGED\n",
    "\n",
    "        # in all of this, n is the dimensions of x\n",
    "        # the purpose of this is to flatten the feature map into a single 1d vector\n",
    "        total_params = (\n",
    "            (x.shape[3] - (self.kernel_size - 1) + self.padding * 2) / self.stride * \n",
    "            (x.shape[2] - (self.kernel_size - 1) + self.padding * 2) / self.stride * \n",
    "            self.out_channels\n",
    "        )\n",
    "\n",
    "        assert total_params.is_integer()\n",
    "\n",
    "        total_params = int(total_params)\n",
    "\n",
    "        u = [capsule(x).view(batch_size, total_params, 1) for capsule in self.capsules]\n",
    "        # stack up output vectors, u, one for each capsule\n",
    "        u = torch.cat(u, dim=-1)\n",
    "        # squashing the stack of vectors\n",
    "        u_squash = self.squash(u)\n",
    "        return u_squash, total_params\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        '''Squashes an input Tensor so it has a magnitude between 0-1.\n",
    "           param input_tensor: a stack of capsule inputs, s_j\n",
    "           return: a stack of normalized, capsule output vectors, v_j \n",
    "           '''\n",
    "        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm) # normalization coeff\n",
    "        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)    \n",
    "        return output_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helpers # to get transpose softmax function\n",
    "\n",
    "# dynamic routing\n",
    "def dynamic_routing(b_ij, u_hat, squash, routing_iterations=3):\n",
    "    '''Performs dynamic routing between two capsule layers.\n",
    "       param b_ij: initial log probabilities that capsule i should be coupled to capsule j\n",
    "       param u_hat: input, weighted capsule vectors, W u\n",
    "       param squash: given, normalizing squash function\n",
    "       param routing_iterations: number of times to update coupling coefficients\n",
    "       return: v_j, output capsule vectors\n",
    "       '''    \n",
    "    # update b_ij, c_ij for number of routing iterations\n",
    "    for iteration in range(routing_iterations):\n",
    "        # softmax calculation of coupling coefficients, c_ij\n",
    "        c_ij = helpers.softmax(b_ij, dim=2)\n",
    "\n",
    "        # calculating total capsule inputs, s_j = sum(c_ij*u_hat)\n",
    "        s_j = (c_ij * u_hat).sum(dim=2, keepdim=True)\n",
    "\n",
    "        # squashing to get a normalized vector output, v_j\n",
    "        v_j = squash(s_j)\n",
    "\n",
    "        # if not on the last iteration, calculate agreement and new b_ij\n",
    "        if iteration < routing_iterations - 1:\n",
    "            # agreement\n",
    "            a_ij = (u_hat * v_j).sum(dim=-1, keepdim=True)\n",
    "            \n",
    "            # new b_ij\n",
    "            b_ij = b_ij + a_ij\n",
    "    \n",
    "    return v_j # return latest v_j\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only CPU available\n"
     ]
    }
   ],
   "source": [
    "# it will also be relevant, in this model, to see if I can train on gpu\n",
    "TRAIN_ON_GPU = torch.cuda.is_available()\n",
    "\n",
    "if(TRAIN_ON_GPU):\n",
    "    print('Training on GPU!')\n",
    "else:\n",
    "    print('Only CPU available')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RENAME TO FinalCaps\n",
    "# THIS CLASS SHOULD HAVE NUM_CAPSULES = TO THE ACTUAL NUMBER OF CLASSES IN THE CLASSIFICATION PROBLEM\n",
    "# HAVE THIS BE AN INPUT VARIABLE AND CHANGE DECODER CLASS ACCORDINGLY\n",
    "# cifar 10 has 10 classes so all good\n",
    "\n",
    "\n",
    "class DigitCaps(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_capsules=10, \n",
    "                 in_channels=8, out_channels=16):\n",
    "        '''Constructs an initial weight matrix, W, and sets class variables.\n",
    "           param num_capsules: number of capsules to create\n",
    "           param previous_layer_nodes: dimension of input capsule vector, default value = 1152\n",
    "           param in_channels: number of capsules in previous layer, default value = 8\n",
    "           param out_channels: dimensions of output capsule vector, default value = 16\n",
    "           '''\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        # setting class variables\n",
    "        self.num_capsules = num_capsules\n",
    "        self.in_channels = in_channels # previous layer's number of capsules\n",
    "        self.out_channels = out_channels\n",
    "        self.W = None\n",
    "        self.previous_layer_nodes = None\n",
    "\n",
    "    def forward(self, u, previous_layer_nodes):\n",
    "        '''Defines the feedforward behavior.\n",
    "           param u: the input; vectors from the previous PrimaryCaps layer\n",
    "           return: a set of normalized, capsule output vectors\n",
    "           '''\n",
    "\n",
    "        if self.W is None:\n",
    "            self.previous_layer_nodes = previous_layer_nodes\n",
    "            # starting out with a randomly initialized weight matrix, W\n",
    "            # these will be the weights connecting the PrimaryCaps and DigitCaps layers\n",
    "            self.W = nn.Parameter(torch.randn(self.num_capsules, previous_layer_nodes, \n",
    "                                            self.in_channels, self.out_channels))\n",
    "        \n",
    "        # adding batch_size dims and stacking all u vectors\n",
    "        u = u[None, :, :, None, :]\n",
    "        # 4D weight matrix\n",
    "        W = self.W[:, None, :, :, :]\n",
    "        \n",
    "        # calculating u_hat = W*u\n",
    "        u_hat = torch.matmul(u, W)\n",
    "\n",
    "        # getting the correct size of b_ij\n",
    "        # setting them all to 0, initially\n",
    "        b_ij = torch.zeros(*u_hat.size())\n",
    "        \n",
    "        # moving b_ij to GPU, if available\n",
    "        if TRAIN_ON_GPU:\n",
    "            b_ij = b_ij.cuda()\n",
    "\n",
    "        # update coupling coefficients and calculate v_j\n",
    "        v_j = dynamic_routing(b_ij, u_hat, self.squash, routing_iterations=3)\n",
    "\n",
    "        return v_j # return final vector outputs\n",
    "    \n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        '''Squashes an input Tensor so it has a magnitude between 0-1.\n",
    "           param input_tensor: a stack of capsule inputs, s_j\n",
    "           return: a stack of normalized, capsule output vectors, v_j\n",
    "           '''\n",
    "        # same squash function as before\n",
    "        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm) # normalization coeff\n",
    "        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)    \n",
    "        return output_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RGBSigmoid(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x) * 255\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_channels, height, width, input_vector_length=16, input_capsules=10, hidden_dim=512):\n",
    "        '''Constructs an series of linear layers + activations.\n",
    "           param input_vector_length: dimension of input capsule vector, default value = 16\n",
    "           param input_capsules: number of capsules in previous layer, default value = 10\n",
    "           param hidden_dim: dimensions of hidden layers, default value = 512\n",
    "           '''\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # calculate input_dim\n",
    "        input_dim = input_vector_length * input_capsules\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        if num_channels == 3:\n",
    "            # define linear layers + activations\n",
    "            self.linear_layers = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim), # first hidden layer\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(hidden_dim, hidden_dim*2), # second, twice as deep\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(hidden_dim*2, num_channels * height * width), \n",
    "                RGBSigmoid() # scaled sigmoid to get values from 0-255\n",
    "                )\n",
    "        else:\n",
    "            self.linear_layers = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim), # first hidden layer\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(hidden_dim, hidden_dim*2), # second, twice as deep\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(hidden_dim*2, num_channels * height * width), \n",
    "                nn.Sigmoid() # scaled sigmoid to get values from 0-255\n",
    "                )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Defines the feedforward behavior.\n",
    "           param x: the input; vectors from the previous DigitCaps layer\n",
    "           return: two things, reconstructed images and the class scores, y\n",
    "           '''\n",
    "        classes = (x ** 2).sum(dim=-1) ** 0.5\n",
    "        classes = F.softmax(classes, dim=-1)\n",
    "        \n",
    "        # find the capsule with the maximum vector length\n",
    "        # here, vector length indicates the probability of a class' existence\n",
    "        _, max_length_indices = classes.max(dim=1)\n",
    "        \n",
    "        # create a sparse class matrix\n",
    "        sparse_matrix = torch.eye(10) # 10 is the number of classes\n",
    "        if TRAIN_ON_GPU:\n",
    "            sparse_matrix = sparse_matrix.cuda()\n",
    "        # get the class scores from the \"correct\" capsule\n",
    "        y = sparse_matrix.index_select(dim=0, index=max_length_indices.data)\n",
    "        \n",
    "        # create reconstructed pixels\n",
    "        x = x * y[:, :, None]\n",
    "        # flatten image into a vector shape (batch_size, vector_dim)\n",
    "        flattened_x = x.contiguous().view(x.shape[0], -1)\n",
    "        # create reconstructed image vectors\n",
    "        reconstructions = self.linear_layers(flattened_x)\n",
    "        \n",
    "        # return reconstructions and the class scores, y\n",
    "        return reconstructions, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsuleNetwork(nn.Module):\n",
    "    def __init__(self, images):\n",
    "        '''Constructs a complete Capsule Network.'''\n",
    "        super(CapsuleNetwork, self).__init__()\n",
    "        self.images = images\n",
    "        print(\"image shape hting\", images.shape[1])\n",
    "        self.conv_layer = ConvLayer(images.shape[1])\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.digit_capsules = DigitCaps()\n",
    "        self.decoder = Decoder(images.shape[1], images.shape[2], images.shape[3])\n",
    "                \n",
    "    def forward(self):\n",
    "        '''Defines the feedforward behavior.\n",
    "           param images: the original CIFAR10 image input data\n",
    "           return: output of DigitCaps layer, reconstructed images, class scores\n",
    "           '''\n",
    "        primary_caps_output, total_params = self.primary_capsules(self.conv_layer(self.images))\n",
    "        caps_output = self.digit_capsules(primary_caps_output, total_params).squeeze().transpose(0,1)\n",
    "        reconstructions, y = self.decoder(caps_output)\n",
    "        return caps_output, reconstructions, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape hting 3\n"
     ]
    }
   ],
   "source": [
    "# instantiate and print net\n",
    "capsule_net = CapsuleNetwork(images)\n",
    "\n",
    "# move model to GPU, if available \n",
    "if TRAIN_ON_GPU:\n",
    "    capsule_net = capsule_net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsuleLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''Constructs a CapsuleLoss module.'''\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "        self.reconstruction_loss = nn.MSELoss(reduction='sum') # cumulative loss, equiv to size_average=False\n",
    "\n",
    "    def forward(self, x, labels, images, reconstructions):\n",
    "        '''Defines how the loss compares inputs.\n",
    "           param x: digit capsule outputs\n",
    "           param labels: \n",
    "           param images: the original CIFAR10 image input data\n",
    "           param reconstructions: reconstructed CIFAR10 image data\n",
    "           return: weighted margin and reconstruction loss, averaged over a batch\n",
    "           '''\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        ##  calculate the margin loss   ##\n",
    "        \n",
    "        # get magnitude of digit capsule vectors, v_c\n",
    "        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n",
    "\n",
    "        # calculate \"correct\" and incorrect loss\n",
    "        left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
    "        right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
    "        \n",
    "        # sum the losses, with a lambda = 0.5\n",
    "        margin_loss = labels * left + 0.5 * (1. - labels) * right\n",
    "        margin_loss = margin_loss.sum()\n",
    "        ##  calculate the reconstruction loss   ##\n",
    "        images = images.view(reconstructions.size()[0], -1)\n",
    "        reconstruction_loss = self.reconstruction_loss(reconstructions, images)\n",
    "\n",
    "        # return a weighted, summed loss, averaged over a batch size\n",
    "        return (margin_loss + 0.0005 * reconstruction_loss) / images.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# custom loss\n",
    "criterion = CapsuleLoss()\n",
    "\n",
    "# Adam optimizer with default params\n",
    "optimizer = optim.Adam(capsule_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(capsule_net, criterion, optimizer, \n",
    "          n_epochs, print_every=300):\n",
    "    '''Trains a capsule network and prints out training batch loss statistics.\n",
    "       Saves model parameters if *validation* loss has decreased.\n",
    "       param capsule_net: trained capsule network\n",
    "       param criterion: capsule loss function\n",
    "       param optimizer: optimizer for updating network weights\n",
    "       param n_epochs: number of epochs to train for\n",
    "       param print_every: batches to print and save training loss, default = 100\n",
    "       return: list of recorded training losses\n",
    "       '''\n",
    "\n",
    "    # track training loss over time\n",
    "    losses = []\n",
    "\n",
    "    # one epoch = one pass over all training data \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        # initialize training loss\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        capsule_net.train() # set to train mode\n",
    "    \n",
    "        # get batches of training image data and targets\n",
    "        for batch_i, (images, target) in enumerate(train_loader):\n",
    "\n",
    "            # reshape and get target class\n",
    "            target = torch.eye(10).index_select(dim=0, index=target)\n",
    "\n",
    "            if TRAIN_ON_GPU:\n",
    "                images, target = images.cuda(), target.cuda()\n",
    "\n",
    "            # zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "            # get model outputs\n",
    "            caps_output, reconstructions, y = capsule_net()\n",
    "            # calculate loss\n",
    "            loss = criterion(caps_output, target, images, reconstructions)\n",
    "            # perform backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() # accumulated training loss\n",
    "            \n",
    "            # print and record training stats\n",
    "            if batch_i != 0 and batch_i % print_every == 0:\n",
    "                avg_train_loss = train_loss/print_every\n",
    "                losses.append(avg_train_loss)\n",
    "                print('Epoch: {} \\tTraining Loss: {:.8f}'.format(epoch, avg_train_loss))\n",
    "                train_loss = 0 # reset accumulated training loss\n",
    "        \n",
    "    return losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 757.31782843\n",
      "Epoch: 1 \tTraining Loss: 0.97462035\n",
      "Epoch: 1 \tTraining Loss: 0.96249412\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_49048/2004516776.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# training for 3 epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapsule_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_49048/1624223673.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(capsule_net, criterion, optimizer, n_epochs, print_every)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# get model outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcapsule_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;31m# calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_49048/2211802409.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m            '''\n\u001b[0;32m     17\u001b[0m         \u001b[0mprimary_caps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary_capsules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mcaps_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigit_capsules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary_caps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mreconstructions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaps_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcaps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstructions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_49048/4010949803.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, u, previous_layer_nodes)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m# update coupling coefficients and calculate v_j\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mv_j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdynamic_routing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_ij\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquash\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouting_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mv_j\u001b[0m \u001b[1;31m# return final vector outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_49048/3382886771.py\u001b[0m in \u001b[0;36mdynamic_routing\u001b[1;34m(b_ij, u_hat, squash, routing_iterations)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrouting_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# softmax calculation of coupling coefficients, c_ij\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mc_ij\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_ij\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# calculating total capsule inputs, s_j = sum(c_ij*u_hat)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\workspace\\Research\\capsule-net\\helpers.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input_tensor, dim)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtransposed_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# calculate softmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0msoftmaxed_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransposed_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransposed_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m# un-transpose result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msoftmaxed_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtransposed_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1678\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1680\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1681\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1682\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training for 3 epochs\n",
    "n_epochs = 1\n",
    "losses = train(capsule_net, criterion, optimizer, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_49048/1301670497.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(capsule_net, test_loader):\n",
    "    '''Prints out test statistics for a given capsule net.\n",
    "       param capsule_net: trained capsule network\n",
    "       param test_loader: test dataloader\n",
    "       return: returns last batch of test image data and corresponding reconstructions\n",
    "       '''\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    \n",
    "    test_loss = 0 # loss tracking\n",
    "\n",
    "    capsule_net.eval() # eval mode\n",
    "\n",
    "    for batch_i, (images, target) in enumerate(test_loader):\n",
    "        target = torch.eye(10).index_select(dim=0, index=target)\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        if TRAIN_ON_GPU:\n",
    "            images, target = images.cuda(), target.cuda()\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        caps_output, reconstructions, y = capsule_net()\n",
    "        # calculate the loss\n",
    "        loss = criterion(caps_output, target, images, reconstructions)\n",
    "        # update average test loss \n",
    "        test_loss += loss.item()\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(y.data.cpu(), 1)\n",
    "        _, target_shape = torch.max(target.data.cpu(), 1)\n",
    "\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target_shape.data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(batch_size):\n",
    "            label = target_shape.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    # avg test loss\n",
    "    avg_test_loss = test_loss/len(test_loader)\n",
    "    print('Test Loss: {:.8f}\\n'.format(avg_test_loss))\n",
    "\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "                str(i), 100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "        else:\n",
    "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * np.sum(class_correct) / np.sum(class_total),\n",
    "        np.sum(class_correct), np.sum(class_total)))\n",
    "    \n",
    "    # return last batch of capsule vectors, images, reconstructions\n",
    "    return caps_output, images, reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.95494144\n",
      "\n",
      "Test Accuracy of     0:  0% ( 0/1000)\n",
      "Test Accuracy of     1:  0% ( 0/1000)\n",
      "Test Accuracy of     2: 94% (945/1000)\n",
      "Test Accuracy of     3:  0% ( 0/1000)\n",
      "Test Accuracy of     4:  0% ( 0/1000)\n",
      "Test Accuracy of     5:  0% ( 0/1000)\n",
      "Test Accuracy of     6:  4% (47/1000)\n",
      "Test Accuracy of     7:  0% ( 0/1000)\n",
      "Test Accuracy of     8:  0% ( 0/1000)\n",
      "Test Accuracy of     9:  0% ( 0/1000)\n",
      "\n",
      "Test Accuracy (Overall):  9% (992/10000)\n"
     ]
    }
   ],
   "source": [
    "# call test function and get reconstructed images\n",
    "caps_output, images, reconstructions = test(capsule_net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def display_images(images, reconstructions):\n",
    "    '''Plot one row of original CIFAR10 images and another row (below) \n",
    "       of their reconstructions.'''\n",
    "    # convert to numpy images\n",
    "    images = images.data.cpu().numpy()\n",
    "    reconstructions = reconstructions.view(-1, images.shape[1], images.shape[2], images.shape[3])\n",
    "    reconstructions = reconstructions.data.cpu().numpy()\n",
    "    \n",
    "    # # plot the first ten input images and then reconstructed images\n",
    "    # fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(26,5))\n",
    "\n",
    "    # # input images on top row, reconstructions on bottom\n",
    "    # for images, row in zip([images, reconstructions], axes):\n",
    "    #     for img, ax in zip(images, row):\n",
    "    #         ax.imshow(np.squeeze(img), cmap='gray')\n",
    "    #         ax.get_xaxis().set_visible(False)\n",
    "    #         ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    for i in range(10):\n",
    "        # plt.imshow(images[i].reshape(32,32,3))\n",
    "        # plt.imshow(reconstructions[i].reshape(32,32,3))\n",
    "        img = Image.fromarray(images[i], 'RGB')\n",
    "        img = Image.fromarray(reconstructions[i], 'RGB')\n",
    "\n",
    "        # img.save('my.png')\n",
    "        img.show()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display original and reconstructed images, in rows\n",
    "display_images(images, reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert data to Tensor *and* perform random affine transformation\n",
    "transform = transforms.Compose(\n",
    "    [transforms.RandomAffine(degrees=30, translate=(0.1,0.1)),\n",
    "     transforms.ToTensor()]\n",
    "    )\n",
    "\n",
    "# test dataset\n",
    "transformed_test_data = datasets.CIFAR10(root='data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "# prepare data loader\n",
    "transformed_test_loader = torch.utils.data.DataLoader(transformed_test_data, \n",
    "                                                      batch_size=batch_size,\n",
    "                                                      num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(transformed_test_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call test function and get reconstructed images\n",
    "_, images, reconstructions = test(capsule_net, transformed_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original input images\n",
    "display_images(images, reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_analysis(capsule_net, x, select_idx=1):\n",
    "    '''Generates perturbed iage reconstructions given some digit capsule outputs.\n",
    "       param capsule_net: trained capsule network\n",
    "       param x: a batch of digit capsule outputs\n",
    "       param select_idx: selects which image in a batch to analyze, default = 1 \n",
    "       return: list of perturbed, reconstructed images\n",
    "       '''\n",
    "    \n",
    "    classes = (x ** 2).sum(dim=-1) ** 0.5\n",
    "    classes = F.softmax(classes, dim=-1)\n",
    "\n",
    "    # find the capsule with the maximum vector length\n",
    "    # here, vector length indicates the probability of a class' existence\n",
    "    _, max_length_indices = classes.max(dim=1)\n",
    "\n",
    "    # create a sparse class matrix\n",
    "    sparse_matrix = torch.eye(10) # 10 is the number of classes\n",
    "    if TRAIN_ON_GPU:\n",
    "        sparse_matrix = sparse_matrix.cuda()\n",
    "    # get the class scores from the \"correct\" capsule\n",
    "    y = sparse_matrix.index_select(dim=0, index=max_length_indices.data)\n",
    "\n",
    "    # create reconstructed pixels\n",
    "    x = x * y[:, :, None]\n",
    "    \n",
    "    # flatten image into a vector shape (batch_size, vector_dim)\n",
    "    flattened_x = x.reshape(x.size(0), -1)\n",
    "    # select a single image from a batch to work with\n",
    "    flattened_x = flattened_x[select_idx]\n",
    "    \n",
    "    # track reconstructed images\n",
    "    reconstructed_ims = []\n",
    "    # values to change *one* vector dimension by\n",
    "    perturb_range = np.arange(-0.25, 0.30, 0.05)\n",
    "    \n",
    "    # iterate through 16 vector dims\n",
    "    for k in range(16):\n",
    "        # create a copy of flattened_x to modify\n",
    "        transformed_x = None\n",
    "\n",
    "        if TRAIN_ON_GPU:\n",
    "            transformed_x = torch.zeros(*flattened_x.size()).cuda()\n",
    "        else:\n",
    "            transformed_x = torch.zeros(*flattened_x.size())\n",
    "            \n",
    "        transformed_x[:] = flattened_x[:]\n",
    "        # iterate through each perturbation value\n",
    "        for j in range(len(perturb_range)):\n",
    "            # for each capsule output\n",
    "            for i in range(10):\n",
    "                transformed_x[k+(16*i)] = flattened_x[k+(16*i)]+perturb_range[j]\n",
    "\n",
    "            # create reconstructed images\n",
    "            reconstructions = capsule_net.decoder.linear_layers(transformed_x)\n",
    "            # reshape into 28x28 image, (batch_size, depth, x, y)\n",
    "            reconstructions = reconstructions.view(-1, 1, 28, 28)\n",
    "            reconstructed_ims.append(reconstructions)\n",
    "    \n",
    "    # return final list of reconstructed ims    \n",
    "    return reconstructed_ims\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call function and get perturbed reconstructions\n",
    "reconstructed_ims = vector_analysis(capsule_net, caps_output, select_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 20)) # define figsize\n",
    "\n",
    "# display all ims\n",
    "for idx in range(len(reconstructed_ims)):\n",
    "    # convert to numpy images\n",
    "    image = reconstructed_ims[idx]\n",
    "    image = image.detach().cpu().numpy()\n",
    "    # display 16 rows of images\n",
    "    ax = fig.add_subplot(16, len(reconstructed_ims)/16, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(image.squeeze(), cmap='gray')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
